{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea221884",
   "metadata": {},
   "source": [
    "\n",
    "# Comprehensive Machine Learning Notebook (20,000 Words)\n",
    "\n",
    "This notebook covers machine learning techniques and algorithms, with detailed explanations, examples, and practical applications.\n",
    "\n",
    "### Table of Contents:\n",
    "1. **Introduction to Machine Learning**\n",
    "2. **Supervised Learning**\n",
    "   - Classification\n",
    "   - Regression\n",
    "3. **Unsupervised Learning**\n",
    "   - Clustering\n",
    "   - Dimensionality Reduction\n",
    "4. **Semi-Supervised Learning**\n",
    "5. **Reinforcement Learning**\n",
    "6. **Anomaly Detection**\n",
    "7. **Case Studies and Real-World Applications**\n",
    "8. **Future Directions**\n",
    "\n",
    "The content will be progressively added, covering each topic in depth.\n",
    "\n",
    "Stay tuned for updates!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df0cd2",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Introduction to Machine Learning\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on building systems that learn from and make decisions based on data. The core idea behind ML is that instead of explicitly programming the system to perform a task, the system can learn patterns and relationships from examples or experience, improving its performance over time.\n",
    "\n",
    "## Why is Machine Learning Important?\n",
    "\n",
    "ML is essential in modern data-driven technologies because it allows systems to adapt, improve, and provide insights without requiring extensive human intervention. Machine learning powers technologies like recommendation systems (e.g., Netflix, Amazon), autonomous vehicles, and personal assistants (e.g., Siri, Google Assistant). \n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "1. **Supervised Learning**: \n",
    "   - The model learns from labeled data (input-output pairs). The goal is to predict the output for unseen inputs based on the learned patterns.\n",
    "   - **Examples**: Classification (identifying if an email is spam or not), Regression (predicting house prices).\n",
    "\n",
    "2. **Unsupervised Learning**: \n",
    "   - The model works with unlabeled data and tries to find patterns or groupings in the data.\n",
    "   - **Examples**: Clustering (grouping customers by purchasing behavior), Dimensionality Reduction (compressing data).\n",
    "\n",
    "3. **Reinforcement Learning**: \n",
    "   - The model interacts with an environment and learns from feedback (rewards and penalties) to optimize its actions.\n",
    "   - **Examples**: Game AI, Robot Navigation.\n",
    "\n",
    "4. **Semi-Supervised Learning**: \n",
    "   - This combines a small amount of labeled data with a large amount of unlabeled data, allowing the model to benefit from both.\n",
    "   - **Examples**: Face Recognition, Website Classification.\n",
    "\n",
    "5. **Anomaly Detection**:\n",
    "   - The task is to identify rare or unusual patterns in the data, which might indicate fraud or errors.\n",
    "   - **Examples**: Credit Card Fraud Detection, Cyber Intrusion.\n",
    "\n",
    "## Machine Learning Workflow\n",
    "\n",
    "1. **Problem Definition**: Identify the problem to be solved and gather requirements.\n",
    "2. **Data Collection**: Collect relevant data needed for the problem.\n",
    "3. **Data Preprocessing**: Clean, normalize, and prepare data for modeling.\n",
    "4. **Modeling**: Choose a machine learning algorithm and train a model on the data.\n",
    "5. **Evaluation**: Test the model's performance on unseen data.\n",
    "6. **Deployment**: Use the model in a production environment to make predictions.\n",
    "7. **Monitoring and Maintenance**: Continuously monitor the model's performance and update as needed.\n",
    "\n",
    "In the next sections, we'll dive into the key types of machine learning in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b4106",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Supervised Learning\n",
    "\n",
    "Supervised learning is a type of machine learning where the model is trained on labeled data, meaning that each input has a corresponding output. The goal is for the model to learn the mapping from inputs to outputs so that it can make predictions on new, unseen data.\n",
    "\n",
    "Supervised learning is broadly classified into two categories:\n",
    "1. **Classification**: The goal is to predict a discrete label or category.\n",
    "2. **Regression**: The goal is to predict a continuous value.\n",
    "\n",
    "## 2.1 Classification\n",
    "\n",
    "Classification is the process of predicting the class or category of a given input based on the learned relationships from the training data.\n",
    "\n",
    "### Key Algorithms for Classification:\n",
    "\n",
    "1. **Logistic Regression**: \n",
    "   - Despite its name, logistic regression is a classification algorithm. It uses the logistic function to output probabilities that can be used to classify inputs.\n",
    "   \n",
    "2. **K-Nearest Neighbors (KNN)**: \n",
    "   - KNN is a non-parametric, instance-based learning algorithm. It classifies a data point based on the majority class among its nearest neighbors.\n",
    "   \n",
    "3. **Support Vector Machines (SVM)**: \n",
    "   - SVM is a powerful classifier that works by finding the hyperplane that best separates the data into different classes.\n",
    "   \n",
    "4. **Decision Trees**: \n",
    "   - Decision trees are simple, interpretable models that recursively split the data into subgroups to predict the class of an input.\n",
    "   \n",
    "5. **Random Forests**: \n",
    "   - Random forests are an ensemble learning technique that combines multiple decision trees to improve classification performance.\n",
    "\n",
    "6. **Neural Networks**: \n",
    "   - Neural networks are powerful models capable of handling complex classification tasks, particularly in the context of deep learning.\n",
    "\n",
    "### 2.1.1 Logistic Regression\n",
    "\n",
    "Logistic Regression is used for binary classification tasks (where there are only two possible classes). It estimates the probability that an instance belongs to a particular class using the sigmoid function:\n",
    "\n",
    "\\[ \\sigma(z) = \f",
    "rac{1}{1 + e^{-z}} \\]\n",
    "\n",
    "Where \\( z = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n \\). The predicted class is determined by the probability threshold (usually 0.5).\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample data\n",
    "X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "y = [0, 0, 1, 1]  # Labels\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "### 2.1.2 K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN classifies data points based on the proximity of its neighbors in the feature space. It is a lazy learning algorithm, meaning that it doesn't learn a model but stores all the training data, making predictions based on the majority vote among the K closest points.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the KNN classifier with K=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"KNN Accuracy: {accuracy_knn * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "### 2.1.3 Support Vector Machine (SVM)\n",
    "\n",
    "SVM works by finding a hyperplane in a high-dimensional space that maximally separates the data points into different classes. For two classes, it tries to maximize the margin between them.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "### 2.1.4 Decision Trees\n",
    "\n",
    "Decision Trees recursively split the data into subgroups based on feature values to predict a target class. The splits are based on the feature that provides the highest information gain.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_dt * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "### 2.1.5 Random Forests\n",
    "\n",
    "Random Forest is an ensemble learning technique that combines multiple decision trees, each trained on a different subset of the data, to improve the accuracy and robustness of the model.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "In the next section, we will cover **Regression** techniques in supervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51f209",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Regression\n",
    "\n",
    "In regression, the task is to predict a continuous output variable based on one or more input variables. Unlike classification, where the output is a discrete label, regression predicts real-valued outcomes.\n",
    "\n",
    "### Key Algorithms for Regression:\n",
    "\n",
    "1. **Linear Regression**:\n",
    "   - A simple yet powerful method that models the relationship between input features and the target variable using a linear equation.\n",
    "   \n",
    "2. **Polynomial Regression**:\n",
    "   - Extends linear regression by introducing polynomial terms to capture non-linear relationships.\n",
    "   \n",
    "3. **Ridge and Lasso Regression**:\n",
    "   - These are regularized versions of linear regression that prevent overfitting by penalizing large coefficients.\n",
    "   \n",
    "4. **Support Vector Regression (SVR)**:\n",
    "   - Similar to support vector machines for classification, SVR finds the best-fitting line within a margin of tolerance.\n",
    "\n",
    "5. **Decision Tree Regression**:\n",
    "   - Similar to decision tree classification, decision tree regression splits the data into smaller regions to make continuous predictions.\n",
    "\n",
    "6. **Random Forest Regression**:\n",
    "   - An ensemble of decision trees that aggregates the predictions of multiple trees to improve accuracy and reduce variance.\n",
    "\n",
    "### 2.2.1 Linear Regression\n",
    "\n",
    "Linear Regression is the simplest form of regression where the relationship between the independent variable(s) and the dependent variable is modeled as a straight line. The goal is to find the line (or hyperplane in higher dimensions) that best fits the data.\n",
    "\n",
    "The equation for a linear regression model is:\n",
    "\\[ y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \\epsilon \\]\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the predicted value,\n",
    "- \\( w_0, w_1, ..., w_n \\) are the model parameters (coefficients),\n",
    "- \\( x_1, x_2, ..., x_n \\) are the input features, and\n",
    "- \\( \\epsilon \\) is the error term.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (X: input, y: target)\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Initialize and train the model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred = lr.predict(X)\n",
    "print(f\"Predicted values: {y_pred}\")\n",
    "```\n",
    "\n",
    "### 2.2.2 Polynomial Regression\n",
    "\n",
    "Polynomial Regression captures non-linear relationships by adding polynomial terms to the features. The model remains linear in the parameters but can fit more complex relationships.\n",
    "\n",
    "The equation for polynomial regression is:\n",
    "\\[ y = w_0 + w_1x + w_2x^2 + ... + w_nx^n + \\epsilon \\]\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 5, 4, 6])\n",
    "\n",
    "# Transform the input to polynomial features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Initialize and train the polynomial regression model\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_poly, y)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred_poly = poly_reg.predict(X_poly)\n",
    "print(f\"Polynomial Regression Predictions: {y_pred_poly}\")\n",
    "```\n",
    "\n",
    "### 2.2.3 Ridge and Lasso Regression\n",
    "\n",
    "Ridge and Lasso are regularization techniques that prevent overfitting by adding a penalty term to the linear regression cost function.\n",
    "\n",
    "- **Ridge Regression**: Adds an L2 penalty (squared magnitude of coefficients).\n",
    "- **Lasso Regression**: Adds an L1 penalty (absolute value of coefficients), which can result in some coefficients being exactly zero (i.e., feature selection).\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Initialize Ridge and Lasso models\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "# Train the models\n",
    "ridge.fit(X, y)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred_ridge = ridge.predict(X)\n",
    "y_pred_lasso = lasso.predict(X)\n",
    "\n",
    "print(f\"Ridge Predictions: {y_pred_ridge}\")\n",
    "print(f\"Lasso Predictions: {y_pred_lasso}\")\n",
    "```\n",
    "\n",
    "### 2.2.4 Support Vector Regression (SVR)\n",
    "\n",
    "Support Vector Regression (SVR) is a regression technique based on the principles of support vector machines (SVM). SVR aims to fit the best line within a given margin, allowing some error but trying to keep it within a defined tolerance level.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize the SVR model\n",
    "svr = SVR(kernel='linear')\n",
    "\n",
    "# Train the model\n",
    "svr.fit(X, y)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred_svr = svr.predict(X)\n",
    "print(f\"SVR Predictions: {y_pred_svr}\")\n",
    "```\n",
    "\n",
    "### 2.2.5 Decision Tree Regression\n",
    "\n",
    "Decision Tree Regression splits the data into smaller and smaller subsets based on feature values, and then predicts the target value by averaging the values in each region.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize and train the Decision Tree model\n",
    "dt_reg = DecisionTreeRegressor()\n",
    "dt_reg.fit(X, y)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred_dt_reg = dt_reg.predict(X)\n",
    "print(f\"Decision Tree Regression Predictions: {y_pred_dt_reg}\")\n",
    "```\n",
    "\n",
    "### 2.2.6 Random Forest Regression\n",
    "\n",
    "Random Forest Regression is an ensemble learning method that combines the predictions of multiple decision trees to improve accuracy and reduce variance.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X, y)\n",
    "\n",
    "# Predict on new data\n",
    "y_pred_rf_reg = rf_reg.predict(X)\n",
    "print(f\"Random Forest Regression Predictions: {y_pred_rf_reg}\")\n",
    "```\n",
    "\n",
    "In the next section, we will explore **Unsupervised Learning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebeee70",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is a type of machine learning where the model is trained on data without labeled outputs. Instead of learning from labeled examples, the algorithm tries to uncover hidden patterns or structures in the data.\n",
    "\n",
    "### Key Techniques in Unsupervised Learning:\n",
    "1. **Clustering**: Grouping similar data points together.\n",
    "2. **Dimensionality Reduction**: Reducing the number of input variables or features while retaining the essential information.\n",
    "\n",
    "## 3.1 Clustering\n",
    "\n",
    "Clustering is the task of dividing a dataset into groups, or clusters, where data points within the same cluster are more similar to each other than to those in other clusters. It is one of the most common tasks in unsupervised learning.\n",
    "\n",
    "### Key Algorithms for Clustering:\n",
    "1. **K-Means**: A simple and widely used clustering algorithm that partitions the data into K clusters.\n",
    "2. **Hierarchical Clustering**: Builds a hierarchy of clusters using a bottom-up or top-down approach.\n",
    "3. **DBSCAN**: Density-Based Spatial Clustering of Applications with Noise, a robust method for finding clusters of arbitrary shape.\n",
    "\n",
    "### 3.1.1 K-Means Clustering\n",
    "\n",
    "K-Means is an iterative algorithm that partitions the dataset into K clusters. It starts by randomly initializing K cluster centroids and assigns data points to the nearest centroid. The centroids are updated iteratively until convergence.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [8, 9], [9, 10], [10, 11]])\n",
    "\n",
    "# Initialize K-Means with K=2\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Cluster assignments and centroids\n",
    "clusters = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(f\"Cluster labels: {clusters}\")\n",
    "print(f\"Centroids: {centroids}\")\n",
    "```\n",
    "\n",
    "### 3.1.2 Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering creates a tree-like structure of nested clusters, also known as a dendrogram. There are two approaches:\n",
    "- **Agglomerative** (bottom-up): Each data point starts as its own cluster, and pairs of clusters are merged iteratively.\n",
    "- **Divisive** (top-down): All data points start in one cluster, and splits are performed iteratively.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(X, 'ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(8, 4))\n",
    "dendrogram(Z)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 3.1.3 DBSCAN (Density-Based Spatial Clustering)\n",
    "\n",
    "DBSCAN is a clustering algorithm that groups together data points that are closely packed, marking points that lie alone in low-density regions as outliers.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Initialize DBSCAN\n",
    "dbscan = DBSCAN(eps=1, min_samples=2)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Cluster labels\n",
    "clusters_dbscan = dbscan.labels_\n",
    "print(f\"DBSCAN Cluster labels: {clusters_dbscan}\")\n",
    "```\n",
    "\n",
    "## 3.2 Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of input variables or features in a dataset, while preserving as much of the original information as possible. It is commonly used for data visualization and to reduce computational complexity.\n",
    "\n",
    "### Key Algorithms for Dimensionality Reduction:\n",
    "1. **Principal Component Analysis (PCA)**: Projects the data onto a lower-dimensional subspace that maximizes the variance.\n",
    "2. **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: A non-linear technique particularly suited for visualizing high-dimensional datasets.\n",
    "\n",
    "### 3.2.1 Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a linear transformation technique that projects the data onto a lower-dimensional space while retaining as much variance as possible.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(f\"PCA-transformed data:\n",
    "{X_pca}\")\n",
    "```\n",
    "\n",
    "### 3.2.2 t-SNE\n",
    "\n",
    "t-SNE is a non-linear dimensionality reduction technique primarily used for data visualization in 2D or 3D spaces.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "print(f\"t-SNE-transformed data:\n",
    "{X_tsne}\")\n",
    "```\n",
    "\n",
    "In the next section, we will explore **Semi-Supervised Learning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0d7b4",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Semi-Supervised Learning\n",
    "\n",
    "Semi-supervised learning is a hybrid approach that combines a small amount of labeled data with a large amount of unlabeled data. This can be especially useful when labeling data is expensive or time-consuming, but a large amount of unlabeled data is readily available.\n",
    "\n",
    "In semi-supervised learning, the goal is to leverage the unlabeled data to improve the model's performance compared to using only the labeled data.\n",
    "\n",
    "### Key Concepts in Semi-Supervised Learning:\n",
    "\n",
    "1. **Self-Training**: The model is initially trained on the small labeled dataset. It then predicts the labels for the unlabeled data, and those confident predictions are added to the labeled dataset for further training.\n",
    "2. **Co-Training**: Two models are trained on different subsets of the feature space. Each model makes predictions on the unlabeled data, and the most confident predictions are added to the training set of the other model.\n",
    "3. **Generative Models**: These models explicitly model the joint probability distribution of the features and labels, allowing them to infer labels for the unlabeled data.\n",
    "4. **Graph-Based Methods**: These methods use a graph structure to represent the relationships between labeled and unlabeled data points.\n",
    "\n",
    "### Applications of Semi-Supervised Learning:\n",
    "\n",
    "- **Text Classification**: Assigning categories to documents with a small labeled dataset and a large unlabeled corpus.\n",
    "- **Image Classification**: Classifying images when labeling is costly.\n",
    "- **Speech Recognition**: Learning to transcribe speech with minimal labeled examples.\n",
    "- **Medical Diagnosis**: Using a small labeled set of patient data to classify diseases.\n",
    "\n",
    "### 4.1 Self-Training\n",
    "\n",
    "Self-training is one of the simplest semi-supervised learning techniques. In self-training, a model is trained on the available labeled data, and then it predicts labels for the unlabeled data. The most confident predictions are added to the labeled set for further training.\n",
    "\n",
    "#### Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Split into labeled and unlabeled data\n",
    "X_labeled, X_unlabeled, y_labeled, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "# Train a classifier on the labeled data\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_labeled, y_labeled)\n",
    "\n",
    "# Predict on the unlabeled data\n",
    "y_unlabeled_pred = clf.predict(X_unlabeled)\n",
    "\n",
    "# Add the most confident predictions to the labeled dataset (simulating self-training)\n",
    "X_new_labeled = X_unlabeled[y_unlabeled_pred == 1][:50]  # Selecting confident predictions\n",
    "y_new_labeled = y_unlabeled_pred[:50]\n",
    "\n",
    "# Retrain the classifier on the expanded labeled dataset\n",
    "X_combined = np.vstack((X_labeled, X_new_labeled))\n",
    "y_combined = np.hstack((y_labeled, y_new_labeled))\n",
    "clf.fit(X_combined, y_combined)\n",
    "\n",
    "# Predict on new data and evaluate\n",
    "y_pred = clf.predict(X)\n",
    "print(f\"Accuracy after self-training: {clf.score(X, y) * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "### 4.2 Co-Training\n",
    "\n",
    "In co-training, two different models are trained on separate feature subsets of the same data. The models make predictions on the unlabeled data, and the most confident predictions from one model are used to train the other.\n",
    "\n",
    "#### Co-Training Example (conceptual):\n",
    "\n",
    "```python\n",
    "# This example illustrates the co-training process\n",
    "\n",
    "# Train two models on different feature subsets (e.g., RandomForest on one subset, SVM on another)\n",
    "model1.fit(X_labeled[:, :10], y_labeled)\n",
    "model2.fit(X_labeled[:, 10:], y_labeled)\n",
    "\n",
    "# Each model predicts on the unlabeled data\n",
    "y_pred1 = model1.predict(X_unlabeled[:, :10])\n",
    "y_pred2 = model2.predict(X_unlabeled[:, 10:])\n",
    "\n",
    "# Add confident predictions from model1 to model2's training set, and vice versa\n",
    "X_new_labeled1 = X_unlabeled[y_pred1 == 1][:50, :10]  # Model1 confident predictions\n",
    "X_new_labeled2 = X_unlabeled[y_pred2 == 1][:50, 10:]  # Model2 confident predictions\n",
    "\n",
    "# Continue co-training process until convergence\n",
    "```\n",
    "\n",
    "### 4.3 Applications\n",
    "\n",
    "Semi-supervised learning has proven highly effective in real-world applications, particularly when labeled data is scarce. Some use cases include:\n",
    "\n",
    "- **Face Recognition**: Leveraging a small set of labeled images with many unlabeled images.\n",
    "- **Medical Imaging**: Reducing the need for labeled medical scans by training with unlabeled scans.\n",
    "- **Targeted Marketing**: Using semi-supervised learning to identify potential customer segments.\n",
    "\n",
    "In the next section, we will explore **Reinforcement Learning**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d756cb",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Reinforcement Learning\n",
    "\n",
    "Reinforcement learning (RL) is a type of machine learning where an agent interacts with an environment and learns to maximize cumulative rewards over time by taking actions and receiving feedback (rewards or penalties). Unlike supervised learning, where the correct output is given, reinforcement learning relies on feedback signals to guide the agent's learning process.\n",
    "\n",
    "### Key Concepts in Reinforcement Learning:\n",
    "\n",
    "1. **Agent**: The learner or decision-maker that interacts with the environment.\n",
    "2. **Environment**: The setting or system with which the agent interacts.\n",
    "3. **State**: The current situation of the environment as observed by the agent.\n",
    "4. **Action**: The set of all possible moves the agent can make.\n",
    "5. **Reward**: Feedback received from the environment after taking an action (positive or negative).\n",
    "6. **Policy**: A strategy that defines the agent's actions based on the current state.\n",
    "7. **Value Function**: The expected cumulative reward of being in a particular state or taking a particular action.\n",
    "8. **Q-Value**: The expected reward for taking a particular action in a given state.\n",
    "\n",
    "### Key Algorithms in Reinforcement Learning:\n",
    "\n",
    "1. **Q-Learning**: A model-free algorithm where the agent learns the value of each action-state pair through trial and error.\n",
    "2. **Deep Q-Networks (DQN)**: Combines Q-learning with deep neural networks to handle large, high-dimensional environments.\n",
    "3. **Policy Gradient Methods**: The agent directly learns a policy that maps states to actions using a probabilistic framework.\n",
    "4. **Actor-Critic Methods**: Combines policy gradients with value-based methods by using two models: an actor (policy) and a critic (value function).\n",
    "\n",
    "### 5.1 Q-Learning\n",
    "\n",
    "Q-Learning is a value-based reinforcement learning algorithm where the agent learns the optimal policy by estimating the value of state-action pairs (Q-values). The agent updates its Q-values iteratively using the Bellman equation.\n",
    "\n",
    "The update rule for Q-learning is:\n",
    "\n",
    "\\[ Q(s, a) \\leftarrow Q(s, a) + \u0007lpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \r",
    "ight] \\]\n",
    "\n",
    "Where:\n",
    "- \\( s \\) is the current state,\n",
    "- \\( a \\) is the current action,\n",
    "- \\( r \\) is the reward received after taking action \\( a \\),\n",
    "- \\( \u0007lpha \\) is the learning rate,\n",
    "- \\( \\gamma \\) is the discount factor (which determines the importance of future rewards),\n",
    "- \\( s' \\) is the next state.\n",
    "\n",
    "#### Example in Python (Q-Learning for GridWorld):\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Initialize parameters\n",
    "states = 5  # Number of states\n",
    "actions = 2  # Number of actions\n",
    "q_table = np.zeros((states, actions))  # Q-table\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "# Define a simple reward function\n",
    "rewards = np.array([0, 0, 0, 0, 1])  # Final state gives a reward of 1\n",
    "\n",
    "# Q-Learning loop (simplified)\n",
    "for episode in range(1000):\n",
    "    state = 0  # Start at state 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.random.choice(actions)  # Random action\n",
    "        next_state = state + 1 if action == 1 else state  # Move to next state if action is 1\n",
    "        reward = rewards[next_state]  # Get reward from next state\n",
    "\n",
    "        # Update Q-values\n",
    "        q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state, :]) - q_table[state, action])\n",
    "\n",
    "        # Transition to next state\n",
    "        state = next_state\n",
    "        if state == 4:  # If final state is reached\n",
    "            done = True\n",
    "\n",
    "print(f\"Learned Q-table:\n",
    "{q_table}\")\n",
    "```\n",
    "\n",
    "### 5.2 Deep Q-Networks (DQN)\n",
    "\n",
    "Deep Q-Networks (DQN) is an extension of Q-learning that uses a deep neural network to approximate the Q-value function. This is particularly useful in environments with high-dimensional state spaces (e.g., video games). The key components of DQN include:\n",
    "\n",
    "- **Experience Replay**: Stores the agent's experiences and samples them randomly to break the correlation between consecutive experiences.\n",
    "- **Target Network**: A separate network that is periodically updated to stabilize learning.\n",
    "\n",
    "#### Example in Python (Pseudocode for DQN):\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the neural network architecture\n",
    "def create_dqn(input_shape, num_actions):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(24, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(layers.Dense(24, activation='relu'))\n",
    "    model.add(layers.Dense(num_actions, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initialize the DQN\n",
    "dqn = create_dqn(input_shape=4, num_actions=2)\n",
    "\n",
    "# Train the DQN (pseudocode)\n",
    "for episode in range(1000):\n",
    "    state = env.reset()  # Reset environment\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Select action using epsilon-greedy policy\n",
    "        action = np.random.choice(actions)\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store experience in replay memory\n",
    "        replay_memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Sample a batch of experiences and update the network\n",
    "        batch = random.sample(replay_memory, batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            target = reward + gamma * np.max(dqn.predict(next_state)) * (1 - done)\n",
    "            target_f = dqn.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            dqn.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        state = next_state\n",
    "```\n",
    "\n",
    "### 5.3 Policy Gradient Methods\n",
    "\n",
    "In policy gradient methods, the agent directly learns a policy that maps states to actions. The policy is often represented as a probability distribution over actions, and the goal is to optimize the policy to maximize the expected reward.\n",
    "\n",
    "The update rule in policy gradient methods is based on the following equation:\n",
    "\n",
    "\\[ \n",
    "abla J(\theta) = \\mathbb{E} \\left[ \n",
    "abla \\log \\pi_\theta(a|s) Q(s, a) \r",
    "ight] \\]\n",
    "\n",
    "Where:\n",
    "- \\( \\pi_\theta(a|s) \\) is the policy (the probability of taking action \\( a \\) in state \\( s \\)),\n",
    "- \\( Q(s, a) \\) is the expected reward for taking action \\( a \\) in state \\( s \\).\n",
    "\n",
    "#### Example in Python (Pseudocode for Policy Gradient):\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a policy network\n",
    "def create_policy_network(input_shape, num_actions):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(24, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(layers.Dense(24, activation='relu'))\n",
    "    model.add(layers.Dense(num_actions, activation='softmax'))  # Softmax output for probabilities\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Initialize the policy network\n",
    "policy_network = create_policy_network(input_shape=4, num_actions=2)\n",
    "\n",
    "# Train the policy network (pseudocode)\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Select action using the policy network\n",
    "        action_probabilities = policy_network.predict(state)\n",
    "        action = np.random.choice(range(len(action_probabilities)), p=action_probabilities)\n",
    "\n",
    "        # Take action in the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Calculate the policy gradient and update the network\n",
    "        with tf.GradientTape() as tape:\n",
    "            log_probs = tf.math.log(policy_network(state)[action])\n",
    "            loss = -log_probs * reward  # Minimize negative reward\n",
    "\n",
    "        gradients = tape.gradient(loss, policy_network.trainable_variables)\n",
    "        policy_network.optimizer.apply_gradients(zip(gradients, policy_network.trainable_variables))\n",
    "\n",
    "        state = next_state\n",
    "```\n",
    "\n",
    "In the next section, we will explore **Anomaly Detection**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
